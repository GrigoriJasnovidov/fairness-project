{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73ea130",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we will take some data and try to solve multiclass classification problem with fairness constraint 'independence'. The data will be taken from fairlearn package; we will try to predict age group of an induvidual \n",
    "using his information. The sensitive feature will be gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "b8b0d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy.optimize\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.datasets import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "id": "82102e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a data set from fairlearn and then prepare it for classification\n",
    "data = fetch_adult(as_frame = True)\n",
    "full_d = data['data']\n",
    "full_d['salary'] = data['target'].map({'<=50K':0, '>50K':1})\n",
    "\n",
    "del full_d['education-num']\n",
    "dum_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship','race','sex','native-country']\n",
    "full_d = pd.get_dummies(full_d,columns = dum_cols,dummy_na = True, drop_first = True)\n",
    "\n",
    "# this function split numerical age into one of three age groups\n",
    "def old(x):\n",
    "    if x <35: \n",
    "        return 0\n",
    "    elif 35<= x<= 55:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "full_d.age = full_d.age.apply(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "id": "6aec15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the data set to get more unbiased results\n",
    "full_d = full_d.sample(full_d.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "63f7f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take 14000 first observations; more observations cannot be taken due the \n",
    "# complexity of the linear-programming problem\n",
    "d = full_d[0:14000]\n",
    "\n",
    "# create train and test observations\n",
    "y = d.drop(['age'],axis=1)\n",
    "x = d['age']\n",
    "y_train,y_test,x_train,x_test = train_test_split(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "id": "ec02437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6306\n",
       "0    5976\n",
       "2    1718\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 1280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of age group\n",
    "d.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "id": "79bba3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4268571428571429"
      ]
     },
     "execution_count": 1281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of the baseline predictor\n",
    "d.age.value_counts()[0]/14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "id": "cc7bf0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6845714285714286"
      ]
     },
     "execution_count": 1282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take some classifier that gives significant improvement against the baseline predictor and\n",
    "# fit it on the data\n",
    "lg = lgb.LGBMClassifier()\n",
    "lg.fit(y_train,x_train)\n",
    "lg_pred= lg.predict(y_test)\n",
    "accuracy_score(lg_pred, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc507d83",
   "metadata": {},
   "source": [
    "# Fairness approach\n",
    "In this section we train the fair classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "id": "cf03ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create male/female train/test features/labels\n",
    "female_train_features = y_train[y_train['sex_Male']==0]\n",
    "male_train_features = y_train[y_train['sex_Male']==1]\n",
    "female_train_labels = x_train[female_train_features.index]\n",
    "male_train_labels = x_train[male_train_features.index]\n",
    "\n",
    "female_test_features = y_test[y_test['sex_Male']==0]\n",
    "male_test_features = y_test[y_test['sex_Male']==1]\n",
    "female_test_labels = x_test[female_test_features.index]\n",
    "male_test_labels = x_test[male_test_features.index]\n",
    "\n",
    "# f,m stand for amount of females and males in the training set\n",
    "f = female_train_features.shape[0]\n",
    "m = male_train_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "id": "1e24aa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex_Male\n",
       "0    0.712212\n",
       "1    0.670674\n",
       "Name: accuracy_score, dtype: object"
      ]
     },
     "execution_count": 1284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the fairness results\n",
    "mf= MetricFrame(metrics=accuracy_score, y_pred=lg_pred, y_true=x_test,sensitive_features = y_test['sex_Male'])\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "id": "8d956048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create vectors of male/female probabilities. \n",
    "#Later on they will be passed as an input to the linear programming problem\n",
    "male_train_probs = lg.predict_proba(male_train_features)\n",
    "female_train_probs = lg.predict_proba(female_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "id": "cd924c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create arguments that will be passed to the linear programming problem\n",
    "m = male_train_probs.shape[0]\n",
    "f = female_train_probs.shape[0]\n",
    "\n",
    "C = male_train_probs.ravel()\n",
    "B = female_train_probs.ravel()\n",
    "objective = (-1)*np.concatenate((C,B))\n",
    "\n",
    "bounds = []\n",
    "for i in range(3*m+3*f):\n",
    "    bounds.append((0,1))\n",
    "\n",
    "equation_vector = [1]*(m+f)\n",
    "for i in range(3):\n",
    "    equation_vector.append(0)\n",
    "\n",
    "equation_matrix = np.zeros((m+f+3,3*f+3*m))\n",
    "for i in range(f+m):\n",
    "    equation_matrix[i,3*i] = 1\n",
    "    equation_matrix[i,3*i+1] = 1\n",
    "    equation_matrix[i,3*i+2] = 1\n",
    "for i in range(3):\n",
    "    for j in range(m):\n",
    "        equation_matrix[f+m+i,3*j+i] = f\n",
    "    for j in range(f):\n",
    "        equation_matrix[f+m+i, 3*m+3*j+i]=-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "id": "80a3b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 1287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#solving linear programm; this is the most computationaly consuming part \n",
    "array = scipy.optimize.linprog(\n",
    "    c = objective, A_ub=None, b_ub=None, \n",
    "                       A_eq=equation_matrix, \n",
    "                       b_eq=equation_vector, \n",
    "    bounds=bounds, method='highs-ipm', callback=None, options=None, x0=None).x\n",
    "array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "id": "3f32ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we create functions that convert 0-1 array obtained above to classes 0,1 and 2 \n",
    "def slice_to_class(l):\n",
    "    if np.array_equal(l,np.array([1,0,0])):\n",
    "        return 0\n",
    "    elif np.array_equal(l,np.array([0,1,0])):\n",
    "        return 1\n",
    "    elif np.array_equal(l,np.array([0,0,1])):\n",
    "        return 2\n",
    "def zeros_ones_to_classes(x,length = 3):\n",
    "    x = np.around(x)\n",
    "    n = int(len(x)/length)\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        z = x[i*length:i*length+length]\n",
    "        l.append(slice_to_class(z))\n",
    "    return np.array(l, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "id": "4d06701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally create vectors of fair predictions\n",
    "fair_pred = zeros_ones_to_classes(array)\n",
    "fair_pred_male = fair_pred[:m]\n",
    "fair_pred_female = fair_pred[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "id": "fe7a6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7615516257843696 0.7523591649985703\n"
     ]
    }
   ],
   "source": [
    "# the results on training set are fair now\n",
    "print(accuracy_score(fair_pred_female,female_train_labels),accuracy_score(fair_pred_male,male_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "id": "18a440f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we prepare classes to relabeling\n",
    "mdf = pd.DataFrame(male_train_probs, columns = ['zero_class', 'first_class','second_class'])\n",
    "male_features_after_classif = mdf.copy()\n",
    "mdf['fair'] = fair_pred_male\n",
    "fdf = pd.DataFrame(female_train_probs, columns = ['zero_class', 'first_class','second_class'])\n",
    "female_features_after_classif = fdf.copy()\n",
    "fdf['fair'] = fair_pred_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "7eca1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create male and female random forest classifiers that will improve fairness\n",
    "m_rf = RandomForestClassifier()\n",
    "m_rf.fit(male_features_after_classif,mdf['fair'])\n",
    "f_rf = RandomForestClassifier()\n",
    "f_rf.fit(female_features_after_classif,fdf['fair']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f25e8e",
   "metadata": {},
   "source": [
    "# Testing on different data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2432c1",
   "metadata": {},
   "source": [
    "In this section we consider all other observations and validate our approach on them. We will take\n",
    "some classifier (Light Gradient Boosting) and then relabel its predictions by the random forests obtained \n",
    "in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "id": "f5310b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the rest of the data\n",
    "validation_data = full_d[14000:]\n",
    "validation_y = validation_data.drop(['age'],axis=1)\n",
    "validation_x = validation_data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "id": "8126a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6849959820916083 5967\n"
     ]
    }
   ],
   "source": [
    "# fit and test LGBMClassifier on the validation data\n",
    "validation_y_train,validation_y_test,validation_x_train,validation_x_test = train_test_split(validation_y,validation_x)\n",
    "validation_lg = lgb.LGBMClassifier()\n",
    "validation_lg.fit(validation_y_train,validation_x_train)\n",
    "validation_lg_pred= validation_lg.predict(validation_y_test)\n",
    "print(accuracy_score(validation_lg_pred, validation_x_test, normalize = True),\n",
    "      accuracy_score(validation_lg_pred, validation_x_test, normalize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "id": "3678a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take male and feale parts of the test set; we need this to evaluate fairness\n",
    "validation_female_test_features = validation_y_test[validation_y_test['sex_Male']==0]\n",
    "validation_male_test_features = validation_y_test[validation_y_test['sex_Male']==1]\n",
    "validation_female_test_labels = validation_x_test[validation_female_test_features.index]\n",
    "validation_male_test_labels = validation_x_test[validation_male_test_features.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "id": "1d34f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6735183281945872 0.7083188304907762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03480050229618892"
      ]
     },
     "execution_count": 1296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compute accuracy on male and female parts; we also compute unfairness, i.e., the \n",
    "# diffrence between male accuracy and female accuracy. This quantity we will try to minimize. \n",
    "validation_male_test_ans = validation_lg.predict(validation_male_test_features)\n",
    "validation_female_test_ans = validation_lg.predict(validation_female_test_features)\n",
    "print(accuracy_score(validation_male_test_ans, validation_male_test_labels, normalize = True),\n",
    "accuracy_score(validation_female_test_ans, validation_female_test_labels, normalize = True))\n",
    "abs(accuracy_score(validation_male_test_ans, validation_male_test_labels, normalize = True)-\n",
    "accuracy_score(validation_female_test_ans, validation_female_test_labels, normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe86d9",
   "metadata": {},
   "source": [
    "Fairness post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "id": "297f9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we obtain probabilities for the test observations\n",
    "validation_male_test_probs = validation_lg.predict_proba(validation_male_test_features)\n",
    "validation_female_test_probs = validation_lg.predict_proba(validation_female_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "id": "d81f78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6615279205207263 0.6950922380786634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.033564317557937096"
      ]
     },
     "execution_count": 1298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply random forests to relabel predictions\n",
    "male_relabeled_pred = m_rf.predict(validation_male_test_probs)\n",
    "female_relabeled_pred = f_rf.predict(validation_female_test_probs)\n",
    "\n",
    "print(accuracy_score(male_relabeled_pred, validation_male_test_labels), \n",
    "accuracy_score(female_relabeled_pred, validation_female_test_labels))\n",
    "\n",
    "# unfairness\n",
    "abs(accuracy_score(male_relabeled_pred, validation_male_test_labels)-\n",
    "accuracy_score(female_relabeled_pred, validation_female_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "id": "a84c20b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5859\n"
     ]
    }
   ],
   "source": [
    "# final accuracy;\n",
    "print(accuracy_score(male_relabeled_pred, validation_male_test_labels, normalize = False)+\n",
    "accuracy_score(female_relabeled_pred, validation_female_test_labels, normalize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "id": "4f84e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the results after 20 trials; \n",
    "# each number in fair array means fair error, i.e., the difference between male acuracy\n",
    "# and female accuracy after post-processing; the numbers in unfair array means the same for\n",
    "# the unfair predictions\n",
    "fair = np.array([52,40,35,34,40,11,26,25,41,7,43,32,29,42,15,47,35,39,35,11])*(1/100)\n",
    "unfair = np.array([46,34,44,51,36,11,39,35,30,21,52,37,39,44,22,53,40,37,45,22])*(1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "025a9f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.389999999999999, 7.38)"
      ]
     },
     "execution_count": 1301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair.sum(), unfair.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbcfa4",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "After some trials it seems that this approach improves fairness measure 'independence' a little bit. The improvement seems to be quite small but very probably it exists. \n",
    "The accuracy typically goes down a litlle bit after post-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
